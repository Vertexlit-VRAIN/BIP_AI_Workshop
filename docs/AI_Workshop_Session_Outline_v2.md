# AI Workshop: From Magic to Mastery (Revised)

## Session Overview

| Detail | Description |
|---|---|
| Duration | 60 minutes |
| Target Audience | University students (Computer Engineering, Economy, Information and Management, Electrical Engineering) |
| Context | Blended Intensive Program (BIP) |
| Prerequisites | None |
| Format | Interactive workshop with icebreaker game and hands-on team challenges |

---

## Part 1: "AI or Human?" Icebreaker (12-15 minutes)

### Goal

Get students curious and engaged by challenging their ability to distinguish AI-generated content from human-made content.

### Setup

- Prepare 5 pairs of outputs across different formats (reduced from 6-8 to keep timing safe).
- For each pair, one is AI-generated and one is human-made.
- Display pairs on screen one at a time.
- Students vote using Mentimeter or Slido. Have colored cards (green = AI, red = Human) as a fallback in case of connectivity issues.

### Suggested Pairs

1. A paragraph of academic writing (real paper vs. ChatGPT/Claude output)
2. Two portraits (real photo vs. Midjourney/DALL-E generation)
3. Two short pieces of code solving the same problem
4. A real financial analyst summary vs. an AI-generated one
5. A real product review vs. an AI-generated fake review

### Flow

- Show each pair, let students vote, then reveal the answer (~2 min per pair = 10 min).
- After each reveal, one sentence on what made it hard to tell. Do not over-explain.
- Keep it conversational, not lecture-style.

### Buffer Activity (if ahead of schedule, +3 min)

Ask 2-3 students: "Which pair surprised you most and why?" Let them answer briefly.

### Transition

Close with the question: "So if AI can do all of this, what does it actually need from you?" This bridges into Part 2.

---

## Part 2: Prompt Engineering Challenge (38 minutes)

### Setup (3 minutes)

- Form teams of 3-4 students, mixing backgrounds deliberately. Each team should have students from at least two different fields.
- Each team needs at least one laptop with access to ChatGPT or Claude.
- Hand out printed handouts with the round tasks and scenario.
- Display the rules: 3 rounds, increasing difficulty. After each round, volunteers share what they found.

---

### Round 1: "Good Prompt vs. Bad Prompt" (10 min)

**Task (6 min):** Every team receives the same block of dense, jargon-heavy text (e.g., an excerpt from the EU AI Act, Article 6 on risk classification). Two steps:

- Step 1: Write the worst prompt you can think of and run it. Save the output.
- Step 2: Write the best prompt you can to produce a clear 5-sentence summary for a general audience. Save the output.

**Sharing:** Ask 2-3 volunteer teams to show their worst and best outputs side by side. Highlight the contrast.

**Debrief (2 min):** Ask a volunteer team: "What did you change between your first and second prompt?"

**Buffer (2 min):** If time allows, ask the room: "What specific words or instructions in your prompt made the biggest difference?"

**Key lesson:** Vague prompts produce vague results. Specificity matters.

---

### Round 2: "The Consulting Team" (15 min)

This round replaces the original field-specific challenges. Instead of separating students by field, it uses a single cross-disciplinary scenario where every background contributes.

**Scenario (given to all teams on screen and on a printed handout):**

> A small company (20 employees, based in Southern Europe) is considering installing solar panels on their 200 m² office roof. The CEO has asked your consulting team to use AI to prepare a quick feasibility brief. Each team member takes one role based on their background:

| Role | Assigned to | Task |
|---|---|---|
| The Engineer | Electrical Engineering student (or whoever volunteers) | Prompt the AI to estimate the system size, panel count, and expected annual energy output for a 200 m² roof in Southern Europe |
| The Economist | Economy student (or whoever volunteers) | Prompt the AI to estimate installation costs, annual savings on electricity, and payback period |
| The Manager | Information and Management student (or whoever volunteers) | Prompt the AI to identify the top 5 project risks and propose one mitigation action for each |
| The Technologist | Computer Engineering student (or whoever volunteers) | Prompt the AI to suggest what data the company should collect to monitor system performance, and how to automate that monitoring |

If a team lacks a student from a given field, someone else takes that role. This is intentional: prompting AI about an unfamiliar domain is harder, which reinforces the lesson about domain knowledge.

**Flow (10 min):**

- Each team member writes ONE prompt from their assigned role (4 min).
- The team reviews all four AI outputs and combines them into a single one-page feasibility brief (4 min).
- 2-3 volunteer teams share highlights from their brief (2 min).

**Debrief (3 min):** Ask: "Where did the AI give you something useful right away? Where did it clearly make something up?" Let the volunteers and other teams answer.

**Buffer (2 min):** Ask teams: "If you had 5 more minutes, which part of the brief would you redo with a better prompt?"

**Key lesson:** AI output quality depends on the domain knowledge guiding the prompt. Each field brings something the AI cannot provide on its own. The best results come from combining human expertise with AI capabilities.

---

### Round 3: "Break the AI" (10 min)

**Task (7 min):** Teams compete to make the AI produce the most spectacular failure. Each team must attempt at least one of the following:

- Get it to confidently state a false fact about your field of study.
- Get it to contradict itself within 3 messages.
- Get it to agree with a clearly wrong premise (e.g., "Water boils at 50°C at sea level, right?").
- Get it to produce a completely useless answer to a reasonable question.

Each team screenshots their best "break" and uploads it to the shared Padlet wall (link displayed on the slide: https://tinyurl.com/bip-ai-work-pad).

**Sharing:** Open the Padlet wall on the projector and scroll through all submissions. As each screenshot comes up, the team that posted it gives a 15-second explanation of what they asked and why the AI failed. After all submissions are shown, the room votes on the most impressive failure using applause or hand-raise.

**Debrief (3 min):** Highlight the top 2-3 failures from the Padlet wall. Briefly explain why these failures happen: the AI was trained on patterns in text, it tends to agree with the user even when the user is wrong (sycophancy), and it has no actual understanding of whether its output is true or false.

**Key lesson:** AI is not a source of truth. Domain expertise is what catches AI mistakes. Always verify.

---

## Part 3: Wrap-up (5-7 minutes)

### Quick Poll (2 min)

One final Mentimeter/Slido question displayed on screen: "After today, I think AI is..."

- More useful than I expected
- More limited than I expected
- About what I expected
- I need to learn more before I decide

Show results live. No commentary needed beyond reading the distribution.

### Three Takeaways (2 min)

Display on screen, read aloud:

1. AI output quality depends on prompt quality. Learning to direct AI is a skill worth practicing.
2. AI sounds confident even when it is wrong. Your domain knowledge is what catches mistakes.
3. The best results come when humans from different fields combine their expertise with AI.

### Buffer: Open Mic (if time remains, +3 min)

Ask: "One thing you plan to try with AI this week?" Take 2-3 volunteer answers from the room.

---

## Timing Summary

| Part | Activity | Core Time | Buffer |
|---|---|---|---|
| 1 | "AI or Human?" Icebreaker | 12 min | +3 min |
| 2 | Team formation and setup | 3 min | — |
| 2.1 | Round 1: Good vs. Bad Prompt | 8 min | +2 min |
| 2.2 | Round 2: The Consulting Team | 13 min | +2 min |
| 2.3 | Round 3: Break the AI | 10 min | — |
| 3 | Wrap-up | 4 min | +3 min |
| | **Core total** | **50 min** | |
| | **Buffer total** | | **+10 min** |
| | **Maximum total** | **60 min** | |

The core activities require approximately 50 minutes. The buffer activities add up to 10 minutes. Use them as needed to reach 60 minutes without rushing. If everything runs smoothly and you still have extra time, extend Round 3 ("Break the AI") because students will fill that time enthusiastically.

---

## Materials Checklist

- [ ] Slide deck with 5 AI-or-Human pairs and reveal slides (Part 1)
- [ ] Voting tool setup (Mentimeter or Slido free tier) and colored cards as fallback
- [ ] Dense text excerpt for Round 1 (e.g., EU AI Act Article 6, ~300 words)
- [ ] Printed handout with the solar panel scenario and role assignments (Round 2)
- [ ] Printed student handouts with round tasks and scenario
- [ ] Padlet board created for Round 3 "Break the AI" screenshot uploads (link on slide: https://tinyurl.com/bip-ai-work-pad)
- [ ] Final Mentimeter poll question pre-loaded (Part 3)
- [ ] Three Takeaways slide ready (Part 3)
- [ ] Students need: one laptop per team, a free ChatGPT or Claude account

---

## Presenter Notes

### On pacing

If a round runs long, skip its buffer activity and move to the next round. If a round finishes early, use the buffer before moving on. The session is designed so that no single overrun breaks the schedule. The 10 minutes of buffer distributed across the session give you flexibility.

### On language

All task instructions should appear on screen or on printed handouts, not only spoken aloud. This helps both you and the students, especially if English is not the first language for anyone in the room. Keep your spoken instructions short and direct. The handouts carry the detail.

### On the "Consulting Team" round

Assign roles based on student backgrounds where possible. If a team has two economists and no engineer, one economist takes the engineer role. This is a feature, not a problem: it demonstrates that prompting AI about an unfamiliar domain is harder, which reinforces the core lesson about domain knowledge.

### On sharing and volunteers

After each round, ask for 2-3 volunteer teams to share their results. Do not force every team to present. If no one volunteers immediately, pick a team that looked engaged during the round. Keep the sharing brief and conversational. The goal is discussion, not evaluation.

### On what changed from v1

- Reduced AI-or-Human pairs from 6-8 to 5 (safer timing, less pressure per pair).
- Reduced prompt engineering rounds from 4 to 3 (removed the standalone "Data Extraction" round to reclaim time).
- Replaced the field-specific Round 3 with a single cross-disciplinary scenario ("The Consulting Team") that makes mixed teams an advantage instead of a problem.
- Added buffer activities at multiple points so you can expand or contract the session without improvising.
- Added a quick poll in the wrap-up to give students a final interactive moment and give you a visual ending.
- Added presenter notes on pacing, language, and scoring.
