# AI Workshop: From Magic to Mastery

## Session Overview

| Detail | Description |
|---|---|
| Duration | 60 minutes |
| Target Audience | University students (Computer Engineering, Economy, Information and Management, Electrical Engineering) |
| Context | Blended Intensive Program (BIP) |
| Prerequisites | None |
| Format | Interactive workshop with icebreaker game and hands-on team challenges |

---

## Part 1: "AI or Human?" Icebreaker (15 minutes)

### Goal

Get students curious and engaged by challenging their ability to distinguish AI-generated content from human-made content.

### Setup

- Prepare 6-8 pairs of outputs across different formats.
- For each pair, one is AI-generated and one is human-made.
- Display pairs on screen one at a time.
- Students vote using Mentimeter, Slido, or hand-raising.

### Suggested Pairs

1. A paragraph of academic writing (real paper vs. ChatGPT/Claude output)
2. Two portraits (real photo vs. Midjourney/DALL-E generation)
3. Two short pieces of code solving the same problem
4. A real financial analyst summary vs. an AI-generated one
5. Two pieces of music (human-composed vs. Suno AI)
6. A real product review vs. an AI-generated fake review

### Flow

- Show each pair, let students vote, then reveal the answer.
- After each reveal, spend ~30 seconds on the clues and what made it hard to tell.
- Keep it conversational, not lecture-style.

### Transition

Close with the question: "So if AI can do all of this, what does it actually need from you?" This bridges into Part 2.

---

## Part 2: Prompt Engineering Challenge (40 minutes)

### Goal

Teach students that working with AI is a skill. The quality of AI output depends on how you direct it.

### Setup

- Form small teams of 3-4 students with mixed backgrounds.
- Each team needs at least one laptop with access to ChatGPT or Claude.
- Prepare a scoring sheet (team name, points per round).
- 4 rounds of increasing difficulty, ~10 minutes each (including debrief).

---

### Round 1: "The Basics" (10 min)

**Task:** Every team receives the same block of dense, jargon-heavy text (e.g., an excerpt from the EU AI Act). They must prompt the AI to produce a clear 5-sentence summary for a general audience.

**Scoring criteria:** Accuracy, clarity, brevity.

**Debrief (2 min):** Show the best and worst prompts side by side. Highlight what made the difference.

**Key lesson:** Vague prompts produce vague results. Specificity matters.

---

### Round 2: "Data Extraction" (10 min)

**Task:** Teams receive a messy, unstructured source (e.g., a screenshot of a restaurant menu, a scanned table from a report, or a long email thread). They must prompt the AI to extract specific structured information into a table format.

**Scoring criteria:** Speed and accuracy of the extracted data.

**Debrief (2 min):** Show how specifying the output structure (format, columns, data types) in the prompt changes the results dramatically.

**Key lesson:** Giving the AI a clear template or format improves output quality.

---

### Round 3: "The Expert" (10 min)

**Task:** Each team picks a challenge related to their field:

| Field | Challenge |
|---|---|
| Computer Engineering | Get the AI to debug a short Python function with 2-3 hidden bugs |
| Economy | Get the AI to analyze 20 rows of fake GDP data and identify a trend |
| Information and Management | Get the AI to write a project risk assessment for a given scenario |
| Electrical Engineering | Get the AI to explain how to size a solar panel system for a specific building |

**Scoring criteria:** Teams score each other's outputs on usefulness and correctness.

**Debrief (2 min):** Ask teams: "Where did the AI get it wrong? How did you catch it?"

**Key lesson:** AI can sound confident while being wrong. Domain knowledge is essential to verify outputs.

---

### Round 4: "Break the AI" (10 min)

**Task:** Teams try to make the AI produce incorrect, contradictory, or absurd outputs. Specific challenges:

- Get it to confidently state a false historical fact
- Get it to contradict itself within the same conversation
- Get it to agree with a clearly wrong premise

**Scoring criteria:** Creativity and severity of the failure produced.

**Debrief (2 min):** Collect and display the best "breaks." Discuss why these failures happen (training data patterns, people-pleasing tendencies, lack of true reasoning).

**Key lesson:** AI is not a source of truth. Always verify.

---

## Part 3: Wrap-up Discussion (5 minutes)

### Flow

No slides. Ask three open questions to the room:

1. "What surprised you today?"
2. "How do you see yourself using this in your specific field?"
3. "What should you NOT trust AI with?"

Let students respond freely.

### Closing Takeaway

AI is a powerful tool, but the quality of what it produces depends entirely on the person directing it. Learning to work with AI effectively is a skill worth developing, regardless of your field.

---

## Materials Checklist

- [ ] Slide deck with AI-or-Human pairs (Part 1)
- [ ] Voting tool set up (Mentimeter/Slido free tier, or plan for hand-raising)
- [ ] Document with all 4 round challenges, ready to share (Google Doc or printed handout)
- [ ] Dense text excerpt for Round 1 (e.g., EU AI Act section)
- [ ] Messy data source for Round 2 (menu screenshot, scanned table, or email thread)
- [ ] Field-specific challenge materials for Round 3 (broken code, GDP dataset, scenario brief, solar panel specs)
- [ ] Scoring sheet (team name, points per round)
- [ ] Students need: one laptop per team, free ChatGPT or Claude account

---

## Timing Summary

| Part | Activity | Duration |
|---|---|---|
| 1 | "AI or Human?" Icebreaker | 15 min |
| 2 | Prompt Engineering Challenge (4 rounds) | 40 min |
| 3 | Wrap-up Discussion | 5 min |
| **Total** | | **60 min** |
